import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
import spacy
from surprise import SVD, Dataset, Reader
import mysql.connector

# Step 1: Natural Language Processing (NLP)
def tokenize_text(text):
    return nltk.word_tokenize(text)

def extract_entities(text):
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(text)
    entities = [(entity.text, entity.label_) for entity in doc.ents]
    return entities

def analyze_sentiment(text):
    sid = SentimentIntensityAnalyzer()
    sentiment_scores = sid.polarity_scores(text)
    return sentiment_scores['compound']

# Step 2: Machine Learning and Recommendation Systems
def train_collaborative_filtering_model(ratings_df):
    reader = Reader(rating_scale=(1, 5))
    data = Dataset.load_from_df(ratings_df[['user_id', 'item_id', 'rating']], reader)
    model = SVD()
    trainset = data.build_full_trainset()
    model.fit(trainset)
    return model

def get_recommendations(model, user_id):
    recommendations = model.get_recommendations(user_id)
    return recommendations

# Step 3: Database Management
def connect_to_database():
    conn = mysql.connector.connect(
        host='localhost',
        user='username',
        password='password',
        database='assistant_db'
    )
    return conn

def retrieve_user_preferences(conn, user_id):
    cursor = conn.cursor()
    query = "SELECT preference FROM user_preferences WHERE user_id = %s"
    cursor.execute(query, (user_id,))
    preferences = cursor.fetchall()
    return preferences

def update_user_preferences(conn, user_id, preference):
    cursor = conn.cursor()
    query = "INSERT INTO user_preferences (user_id, preference) VALUES (%s, %s)"
    cursor.execute(query, (user_id, preference))
    conn.commit()

def close_database_connection(conn):
    conn.close()

# Step 4: User Interaction and Interface
def process_user_input(user_input):
    # Step 1: NLP - Tokenization
    tokens = tokenize_text(user_input)
    
    # Step 1: NLP - Entity Extraction
    entities = extract_entities(user_input)

    # Step 1: NLP - Sentiment Analysis
    sentiment_score = analyze_sentiment(user_input)

    # Step 3: Database Management - Store user input and sentiment score
    conn = connect_to_database()
    update_user_preferences(conn, user_id, sentiment_score)
    close_database_connection(conn)

    # Step 4: User Interaction - Provide response
    print("Assistant: Tokens -", tokens)
    print("Assistant: Entities -", entities)
    print("Assistant: Sentiment Score -", sentiment_score)

# Step 5: Continuous Learning and Database Updates
def handle_user_feedback(user_feedback, conn, user_id):
    if user_feedback.lower() == 'yes':
        preference = 'positive'
    else:
        preference = 'negative'
    update_user_preferences(conn, user_id, preference)

# Sample usage
def main():
    # Sample data
    ratings_df = pd.read_csv('ratings.csv')

    # Step 2: Machine Learning - Train collaborative filtering model
    collaborative_model = train_collaborative_filtering_model(ratings_df)

    while True:
        user_input = input("Enter your query: ")

        # Step 4: User Interaction and Interface
        process_user_input(user_input)

        # Step 5: Continuous Learning and Database Updates
        user_feedback = input("Did you find the recommendation helpful? (Yes/No): ")
        conn = connect_to_database()
        handle_user_feedback(user_feedback, conn, user_id)
        close_database_connection(conn)

if __name__ == "__main__":
    main()
